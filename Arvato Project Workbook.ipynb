{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "If you do not have the required **data/** directory in your workspace, follow the instructions below. Use either one of the methods below. \n",
    "\n",
    "**Method 1** <br/>\n",
    "You must [download this dataset](https://video.udacity-data.com/topher/2024/May/66393287_arvato_data.tar/arvato_data.tar.gz) from the classroom, and upload it into the workspace. After you upload the tar file to the appropriate directory, **/cd0549_bertelsmann_arvato_project_workspace/**,  in the Jupyter server, you can open a terminal and the run the following command to extract the dataset from the compressed file. \n",
    "```bash\n",
    "!tar -xzvf arvato_data.tar.gz\n",
    "```\n",
    "This command will extract all the contents of arvato_data.tar.gz into the current directory from where you ran the command. \n",
    "\n",
    "**Method 2** <br/>\n",
    "Execute the Python code below to download the dataset. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "\n",
    "def download_and_extract(url, extract_to='.'):\n",
    "    \"\"\"\n",
    "    Downloads a tar.gz file from a URL and extracts it to a directory.\n",
    "    Args:\n",
    "    - url (str): URL of the tar.gz file to download.\n",
    "    - extract_to (str): Directory path to extract the contents of the tar.gz file.\n",
    "    \"\"\"\n",
    "    # Get the filename from the URL\n",
    "    filename = url.split('/')[-1]\n",
    "\n",
    "    # Download the file\n",
    "    print(\"Downloading the file...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.raw.read())\n",
    "        print(\"Download completed.\")\n",
    "    else:\n",
    "        print(\"Failed to download the file.\")\n",
    "        return\n",
    "\n",
    "    # Extract the tar.gz file\n",
    "    print(\"Extracting the file...\")\n",
    "    try:\n",
    "        with tarfile.open(filename, 'r:gz') as tar:\n",
    "            tar.extractall(path=extract_to)\n",
    "        print(\"Extraction completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract the file: {e}\")\n",
    "    finally:\n",
    "        # Optionally remove the tar.gz file after extraction\n",
    "        os.remove(filename)\n",
    "        print(\"Downloaded tar.gz file removed.\")\n",
    "\n",
    "# URL of the tar.gz file\n",
    "url = 'https://video.udacity-data.com/topher/2024/May/66393287_arvato_data.tar/arvato_data.tar.gz'\n",
    "\n",
    "# Call the function with the URL\n",
    "download_and_extract(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# magic word for producing visualizations in notebook\u001b[39;00m\n\u001b[32m      8\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mmatplotlib\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minline\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/55cdq9mj5msby1f05d6s0r3m0000gp/T/ipykernel_10434/4183925281.py:2: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';')\n",
      "/var/folders/x9/55cdq9mj5msby1f05d6s0r3m0000gp/T/ipykernel_10434/4183925281.py:3: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  customers = pd.read_csv('data/Udacity_CUSTOMERS_052018.csv', sep=';')\n"
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "customers = pd.read_csv('data/Udacity_CUSTOMERS_052018.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNR</th>\n",
       "      <th>AGER_TYP</th>\n",
       "      <th>AKT_DAT_KL</th>\n",
       "      <th>ALTER_HH</th>\n",
       "      <th>ALTER_KIND1</th>\n",
       "      <th>ALTER_KIND2</th>\n",
       "      <th>ALTER_KIND3</th>\n",
       "      <th>ALTER_KIND4</th>\n",
       "      <th>ALTERSKATEGORIE_FEIN</th>\n",
       "      <th>ANZ_HAUSHALTE_AKTIV</th>\n",
       "      <th>...</th>\n",
       "      <th>VHN</th>\n",
       "      <th>VK_DHT4A</th>\n",
       "      <th>VK_DISTANZ</th>\n",
       "      <th>VK_ZG11</th>\n",
       "      <th>W_KEIT_KIND_HH</th>\n",
       "      <th>WOHNDAUER_2008</th>\n",
       "      <th>WOHNLAGE</th>\n",
       "      <th>ZABEOTYP</th>\n",
       "      <th>ANREDE_KZ</th>\n",
       "      <th>ALTERSKATEGORIE_GROB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910215</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>910220</td>\n",
       "      <td>-1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>910225</td>\n",
       "      <td>-1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910226</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>910241</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 366 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LNR  AGER_TYP  AKT_DAT_KL  ALTER_HH  ALTER_KIND1  ALTER_KIND2  \\\n",
       "0  910215        -1         NaN       NaN          NaN          NaN   \n",
       "1  910220        -1         9.0       0.0          NaN          NaN   \n",
       "2  910225        -1         9.0      17.0          NaN          NaN   \n",
       "3  910226         2         1.0      13.0          NaN          NaN   \n",
       "4  910241        -1         1.0      20.0          NaN          NaN   \n",
       "\n",
       "   ALTER_KIND3  ALTER_KIND4  ALTERSKATEGORIE_FEIN  ANZ_HAUSHALTE_AKTIV  ...  \\\n",
       "0          NaN          NaN                   NaN                  NaN  ...   \n",
       "1          NaN          NaN                  21.0                 11.0  ...   \n",
       "2          NaN          NaN                  17.0                 10.0  ...   \n",
       "3          NaN          NaN                  13.0                  1.0  ...   \n",
       "4          NaN          NaN                  14.0                  3.0  ...   \n",
       "\n",
       "   VHN  VK_DHT4A  VK_DISTANZ  VK_ZG11  W_KEIT_KIND_HH  WOHNDAUER_2008  \\\n",
       "0  NaN       NaN         NaN      NaN             NaN             NaN   \n",
       "1  4.0       8.0        11.0     10.0             3.0             9.0   \n",
       "2  2.0       9.0         9.0      6.0             3.0             9.0   \n",
       "3  0.0       7.0        10.0     11.0             NaN             9.0   \n",
       "4  2.0       3.0         5.0      4.0             2.0             9.0   \n",
       "\n",
       "   WOHNLAGE ZABEOTYP ANREDE_KZ ALTERSKATEGORIE_GROB  \n",
       "0       NaN        3         1                    2  \n",
       "1       4.0        5         2                    1  \n",
       "2       2.0        5         2                    3  \n",
       "3       7.0        3         2                    4  \n",
       "4       3.0        4         1                    3  \n",
       "\n",
       "[5 rows x 366 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully!\n",
      "\n",
      "============================================================\n",
      "DATASET SHAPES\n",
      "============================================================\n",
      "AZDIAS: (1000, 366)\n",
      "CUSTOMERS: (1000, 369)\n",
      "\n",
      "============================================================\n",
      "FIRST 5 ROWS - FIRST 10 COLUMNS (AZDIAS)\n",
      "============================================================\n",
      "      LNR  AGER_TYP  AKT_DAT_KL  ALTER_HH  ALTER_KIND1  ALTER_KIND2  \\\n",
      "0  910215        -1         NaN       NaN          NaN          NaN   \n",
      "1  910220        -1         9.0       0.0          NaN          NaN   \n",
      "2  910225        -1         9.0      17.0          NaN          NaN   \n",
      "3  910226         2         1.0      13.0          NaN          NaN   \n",
      "4  910241        -1         1.0      20.0          NaN          NaN   \n",
      "\n",
      "   ALTER_KIND3  ALTER_KIND4  ALTERSKATEGORIE_FEIN  ANZ_HAUSHALTE_AKTIV  \n",
      "0          NaN          NaN                   NaN                  NaN  \n",
      "1          NaN          NaN                  21.0                 11.0  \n",
      "2          NaN          NaN                  17.0                 10.0  \n",
      "3          NaN          NaN                  13.0                  1.0  \n",
      "4          NaN          NaN                  14.0                  3.0  \n",
      "\n",
      "============================================================\n",
      "DATA TYPES\n",
      "============================================================\n",
      "float64    269\n",
      "int64       93\n",
      "object       4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "MISSING VALUES (Top 20 columns with most missing)\n",
      "============================================================\n",
      "ALTER_KIND4                    1000\n",
      "ALTER_KIND3                     992\n",
      "ALTER_KIND2                     974\n",
      "ALTER_KIND1                     907\n",
      "EXTSEL992                       772\n",
      "KK_KUNDENTYP                    706\n",
      "ALTERSKATEGORIE_FEIN            341\n",
      "D19_VERSAND_ONLINE_QUOTE_12     330\n",
      "D19_LOTTO                       330\n",
      "D19_BANKEN_ONLINE_QUOTE_12      330\n",
      "D19_LETZTER_KAUF_BRANCHE        330\n",
      "D19_SOZIALES                    330\n",
      "D19_GESAMT_ONLINE_QUOTE_12      330\n",
      "D19_KONSUMTYP                   330\n",
      "D19_VERSI_ONLINE_QUOTE_12       330\n",
      "D19_TELKO_ONLINE_QUOTE_12       330\n",
      "KBA05_DIESEL                    197\n",
      "KBA05_CCM4                      197\n",
      "KBA05_GBZ                       197\n",
      "KBA05_FRAU                      197\n",
      "dtype: int64\n",
      "\n",
      "============================================================\n",
      "SAMPLE VALUES FROM FIRST 10 COLUMNS\n",
      "============================================================\n",
      "\n",
      "LNR: [910215 910220 910225 910226 910241 910244 910248 910261 645145 645153]\n",
      "\n",
      "AGER_TYP: [-1  2  3  0  1]\n",
      "\n",
      "AKT_DAT_KL: [nan  9.  1.  5.  8.  7.  6.  4.  3.  2.]\n",
      "\n",
      "ALTER_HH: [nan  0. 17. 13. 20. 10. 14. 16. 21. 11.]\n",
      "\n",
      "ALTER_KIND1: [nan 17. 10. 18. 13. 16. 11.  6.  8.  9.]\n",
      "\n",
      "ALTER_KIND2: [nan 13.  8. 12. 10.  7. 16. 15. 14. 17.]\n",
      "\n",
      "ALTER_KIND3: [nan 10. 18. 17. 16.  8.]\n",
      "\n",
      "ALTER_KIND4: [nan]\n",
      "\n",
      "ALTERSKATEGORIE_FEIN: [nan 21. 17. 13. 14. 10. 16. 20. 11. 19.]\n",
      "\n",
      "ANZ_HAUSHALTE_AKTIV: [nan 11. 10.  1.  3.  5.  4.  6.  2.  9.]\n",
      "\n",
      "============================================================\n",
      "CHECKING FOR SPECIAL MISSING CODES\n",
      "============================================================\n",
      "AGER_TYP: Contains -1\n",
      "AGER_TYP: Contains 0\n",
      "ALTER_HH: Contains 0\n",
      "ALTERSKATEGORIE_FEIN: Contains 0\n",
      "ANZ_HAUSHALTE_AKTIV: Contains 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading data...\")\n",
    "azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';', nrows=5000)  # Load first 1000 for speed\n",
    "customers = pd.read_csv('data/Udacity_CUSTOMERS_052018.csv', sep=';', nrows=5000)\n",
    "\n",
    "print(\"Data loaded successfully!\\n\")\n",
    "\n",
    "# Basic exploration\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET SHAPES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AZDIAS: {azdias.shape}\")\n",
    "print(f\"CUSTOMERS: {customers.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FIRST 5 ROWS - FIRST 10 COLUMNS (AZDIAS)\")\n",
    "print(\"=\"*60)\n",
    "print(azdias.iloc[:5, :10])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\"*60)\n",
    "print(azdias.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES (Top 20 columns with most missing)\")\n",
    "print(\"=\"*60)\n",
    "missing = azdias.isnull().sum().sort_values(ascending=False)\n",
    "print(missing.head(20))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE VALUES FROM FIRST 10 COLUMNS\")\n",
    "print(\"=\"*60)\n",
    "for col in azdias.columns[:10]:\n",
    "    print(f\"\\n{col}: {azdias[col].unique()[:10]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING FOR SPECIAL MISSING CODES\")\n",
    "print(\"=\"*60)\n",
    "# Check for common missing value indicators\n",
    "for col in azdias.columns[:10]:\n",
    "    if azdias[col].dtype in ['int64', 'float64']:\n",
    "        if -1 in azdias[col].values:\n",
    "            print(f\"{col}: Contains -1\")\n",
    "        if 0 in azdias[col].values:\n",
    "            print(f\"{col}: Contains 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "======================================================================\n",
      "STEP 1: IDENTIFY OBJECT (STRING) COLUMNS\n",
      "======================================================================\n",
      "Object columns: ['CAMEO_DEU_2015', 'CAMEO_DEUG_2015', 'CAMEO_INTL_2015', 'D19_LETZTER_KAUF_BRANCHE', 'EINGEFUEGT_AM', 'OST_WEST_KZ']\n",
      "\n",
      "CAMEO_DEU_2015:\n",
      "  Unique values: [nan '8A' '4C' '2A' '6B' '8C' '4A' '2D' '1A' '1E' '9D' '5C' '8B' '7A' '5D'\n",
      " '9E' '9B' '1B' '3D' '4E']\n",
      "  Value counts:\n",
      "CAMEO_DEU_2015\n",
      "6B    313\n",
      "4C    264\n",
      "8A    262\n",
      "8B    199\n",
      "7A    191\n",
      "2D    179\n",
      "4A    177\n",
      "3C    176\n",
      "3D    171\n",
      "9D    160\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CAMEO_DEUG_2015:\n",
      "  Unique values: [nan 8.0 4.0 2.0 6.0 1.0 9.0 5.0 7.0 3.0 '4' '3' '7' '2' '8' '9' '6' '5'\n",
      " '1' 'X']\n",
      "  Value counts:\n",
      "CAMEO_DEUG_2015\n",
      "8      439\n",
      "9      366\n",
      "6      346\n",
      "4      324\n",
      "8.0    273\n",
      "3      267\n",
      "2      267\n",
      "7      261\n",
      "4.0    243\n",
      "6.0    230\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CAMEO_INTL_2015:\n",
      "  Unique values: [nan 51.0 24.0 12.0 43.0 54.0 22.0 14.0 13.0 15.0 33.0 41.0 34.0 55.0 25.0\n",
      " 23.0 31.0 52.0 35.0 45.0]\n",
      "  Value counts:\n",
      "CAMEO_INTL_2015\n",
      "51      434\n",
      "41      322\n",
      "24      287\n",
      "51.0    268\n",
      "41.0    215\n",
      "24.0    199\n",
      "14      197\n",
      "43      178\n",
      "54      140\n",
      "43.0    135\n",
      "Name: count, dtype: int64\n",
      "\n",
      "D19_LETZTER_KAUF_BRANCHE:\n",
      "  Unique values: [nan 'D19_UNBEKANNT' 'D19_SCHUHE' 'D19_ENERGIE' 'D19_KOSMETIK'\n",
      " 'D19_VOLLSORTIMENT' 'D19_SONSTIGE' 'D19_BANKEN_GROSS'\n",
      " 'D19_DROGERIEARTIKEL' 'D19_HANDWERK' 'D19_BUCH_CD' 'D19_VERSICHERUNGEN'\n",
      " 'D19_VERSAND_REST' 'D19_TELKO_REST' 'D19_BANKEN_DIREKT' 'D19_BANKEN_REST'\n",
      " 'D19_FREIZEIT' 'D19_LEBENSMITTEL' 'D19_HAUS_DEKO' 'D19_BEKLEIDUNG_REST']\n",
      "  Value counts:\n",
      "D19_LETZTER_KAUF_BRANCHE\n",
      "D19_UNBEKANNT          1069\n",
      "D19_VERSICHERUNGEN      298\n",
      "D19_SONSTIGE            226\n",
      "D19_VOLLSORTIMENT       201\n",
      "D19_SCHUHE              175\n",
      "D19_VERSAND_REST        150\n",
      "D19_BUCH_CD             140\n",
      "D19_DROGERIEARTIKEL     125\n",
      "D19_BANKEN_DIREKT       123\n",
      "D19_BEKLEIDUNG_REST     114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "EINGEFUEGT_AM:\n",
      "  Unique values: [nan '1992-02-10 00:00:00' '1992-02-12 00:00:00' '1997-04-21 00:00:00'\n",
      " '2005-12-30 00:00:00' '2009-01-19 00:00:00' '1995-02-02 00:00:00'\n",
      " '1996-12-05 00:00:00' '2002-03-13 00:00:00' '2015-04-01 00:00:00'\n",
      " '1996-05-09 00:00:00' '2005-04-12 00:00:00' '2004-10-14 00:00:00'\n",
      " '1996-10-28 00:00:00' '1996-10-30 00:00:00' '2000-05-10 00:00:00'\n",
      " '1996-03-26 00:00:00' '1997-07-29 00:00:00' '2008-02-14 00:00:00'\n",
      " '1997-07-30 00:00:00']\n",
      "  Value counts:\n",
      "EINGEFUEGT_AM\n",
      "1992-02-10 00:00:00    2044\n",
      "1992-02-12 00:00:00     993\n",
      "1995-02-07 00:00:00      51\n",
      "2003-11-18 00:00:00      42\n",
      "2005-12-16 00:00:00      36\n",
      "2000-05-10 00:00:00      17\n",
      "1993-03-01 00:00:00      15\n",
      "1995-10-17 00:00:00      14\n",
      "1994-02-03 00:00:00      14\n",
      "1992-02-21 00:00:00      14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "OST_WEST_KZ:\n",
      "  Unique values: [nan 'W' 'O']\n",
      "  Value counts:\n",
      "OST_WEST_KZ\n",
      "W    3331\n",
      "O     936\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "STEP 2: CHECK FOR -1, 0, X PATTERNS (Potential Missing Codes)\n",
      "======================================================================\n",
      "Found 23 columns with suspicious values:\n",
      "  AGER_TYP: -1 appears 3896 times (77.9%)\n",
      "  ANZ_HH_TITEL: 0 appears 4103 times (82.1%)\n",
      "  ANZ_KINDER: 0 appears 3914 times (78.3%)\n",
      "  ANZ_TITEL: 0 appears 4365 times (87.3%)\n",
      "  D19_BANKEN_ANZ_12: 0 appears 4656 times (93.1%)\n",
      "  D19_BANKEN_ANZ_24: 0 appears 4455 times (89.1%)\n",
      "  D19_BANKEN_DIREKT: 0 appears 4121 times (82.4%)\n",
      "  D19_BANKEN_GROSS: 0 appears 4424 times (88.5%)\n",
      "  D19_BANKEN_LOKAL: 0 appears 4920 times (98.4%)\n",
      "  D19_BANKEN_ONLINE_QUOTE_12: 0 appears 3128 times (62.6%)\n",
      "  D19_BANKEN_REST: 0 appears 4609 times (92.2%)\n",
      "  D19_BEKLEIDUNG_GEH: 0 appears 4571 times (91.4%)\n",
      "  D19_BEKLEIDUNG_REST: 0 appears 3924 times (78.5%)\n",
      "  D19_BILDUNG: 0 appears 4617 times (92.3%)\n",
      "  D19_BIO_OEKO: 0 appears 4814 times (96.3%)\n",
      "  D19_BUCH_CD: 0 appears 3591 times (71.8%)\n",
      "  D19_DIGIT_SERV: 0 appears 4819 times (96.4%)\n",
      "  D19_DROGERIEARTIKEL: 0 appears 4298 times (86.0%)\n",
      "  D19_ENERGIE: 0 appears 4676 times (93.5%)\n",
      "  D19_FREIZEIT: 0 appears 4462 times (89.2%)\n",
      "\n",
      "======================================================================\n",
      "STEP 3: MISSING VALUE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Columns with >50% missing values: 6\n",
      "Columns with >80% missing values: 4\n",
      "\n",
      "Top 30 columns by missing percentage:\n",
      "                                                  column  missing_count  \\\n",
      "ALTER_KIND4                                  ALTER_KIND4           4999   \n",
      "ALTER_KIND3                                  ALTER_KIND3           4963   \n",
      "ALTER_KIND2                                  ALTER_KIND2           4839   \n",
      "ALTER_KIND1                                  ALTER_KIND1           4568   \n",
      "EXTSEL992                                      EXTSEL992           3754   \n",
      "KK_KUNDENTYP                                KK_KUNDENTYP           3365   \n",
      "ALTERSKATEGORIE_FEIN                ALTERSKATEGORIE_FEIN           1680   \n",
      "D19_SOZIALES                                D19_SOZIALES           1609   \n",
      "D19_LOTTO                                      D19_LOTTO           1609   \n",
      "D19_BANKEN_ONLINE_QUOTE_12    D19_BANKEN_ONLINE_QUOTE_12           1609   \n",
      "D19_GESAMT_ONLINE_QUOTE_12    D19_GESAMT_ONLINE_QUOTE_12           1609   \n",
      "D19_KONSUMTYP                              D19_KONSUMTYP           1609   \n",
      "D19_LETZTER_KAUF_BRANCHE        D19_LETZTER_KAUF_BRANCHE           1609   \n",
      "D19_TELKO_ONLINE_QUOTE_12      D19_TELKO_ONLINE_QUOTE_12           1609   \n",
      "D19_VERSAND_ONLINE_QUOTE_12  D19_VERSAND_ONLINE_QUOTE_12           1609   \n",
      "D19_VERSI_ONLINE_QUOTE_12      D19_VERSI_ONLINE_QUOTE_12           1609   \n",
      "KBA05_MAXVORB                              KBA05_MAXVORB            947   \n",
      "KBA05_MAXAH                                  KBA05_MAXAH            947   \n",
      "KBA05_MAXSEG                                KBA05_MAXSEG            947   \n",
      "KBA05_MOD1                                    KBA05_MOD1            947   \n",
      "KBA05_MAXHERST                            KBA05_MAXHERST            947   \n",
      "KBA05_MAXBJ                                  KBA05_MAXBJ            947   \n",
      "KBA05_KW1                                      KBA05_KW1            947   \n",
      "KBA05_KW3                                      KBA05_KW3            947   \n",
      "KBA05_KW2                                      KBA05_KW2            947   \n",
      "KBA05_KRSVAN                                KBA05_KRSVAN            947   \n",
      "KBA05_KRSOBER                              KBA05_KRSOBER            947   \n",
      "KBA05_KRSKLEIN                            KBA05_KRSKLEIN            947   \n",
      "KBA05_KRSHERST3                          KBA05_KRSHERST3            947   \n",
      "KBA05_KRSHERST2                          KBA05_KRSHERST2            947   \n",
      "\n",
      "                             missing_percent  \n",
      "ALTER_KIND4                            99.98  \n",
      "ALTER_KIND3                            99.26  \n",
      "ALTER_KIND2                            96.78  \n",
      "ALTER_KIND1                            91.36  \n",
      "EXTSEL992                              75.08  \n",
      "KK_KUNDENTYP                           67.30  \n",
      "ALTERSKATEGORIE_FEIN                   33.60  \n",
      "D19_SOZIALES                           32.18  \n",
      "D19_LOTTO                              32.18  \n",
      "D19_BANKEN_ONLINE_QUOTE_12             32.18  \n",
      "D19_GESAMT_ONLINE_QUOTE_12             32.18  \n",
      "D19_KONSUMTYP                          32.18  \n",
      "D19_LETZTER_KAUF_BRANCHE               32.18  \n",
      "D19_TELKO_ONLINE_QUOTE_12              32.18  \n",
      "D19_VERSAND_ONLINE_QUOTE_12            32.18  \n",
      "D19_VERSI_ONLINE_QUOTE_12              32.18  \n",
      "KBA05_MAXVORB                          18.94  \n",
      "KBA05_MAXAH                            18.94  \n",
      "KBA05_MAXSEG                           18.94  \n",
      "KBA05_MOD1                             18.94  \n",
      "KBA05_MAXHERST                         18.94  \n",
      "KBA05_MAXBJ                            18.94  \n",
      "KBA05_KW1                              18.94  \n",
      "KBA05_KW3                              18.94  \n",
      "KBA05_KW2                              18.94  \n",
      "KBA05_KRSVAN                           18.94  \n",
      "KBA05_KRSOBER                          18.94  \n",
      "KBA05_KRSKLEIN                         18.94  \n",
      "KBA05_KRSHERST3                        18.94  \n",
      "KBA05_KRSHERST2                        18.94  \n",
      "\n",
      "======================================================================\n",
      "STEP 4: COMPARE AZDIAS vs CUSTOMERS\n",
      "======================================================================\n",
      "Extra columns in CUSTOMERS: {'CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP'}\n",
      "\n",
      "CUSTOMER_GROUP:\n",
      "CUSTOMER_GROUP\n",
      "MULTI_BUYER     3391\n",
      "SINGLE_BUYER    1609\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ONLINE_PURCHASE:\n",
      "ONLINE_PURCHASE\n",
      "0    4555\n",
      "1     445\n",
      "Name: count, dtype: int64\n",
      "\n",
      "PRODUCT_GROUP:\n",
      "PRODUCT_GROUP\n",
      "COSMETIC_AND_FOOD    2591\n",
      "FOOD                 1232\n",
      "COSMETIC             1177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "STEP 5: VISUALIZE MISSING DATA PATTERN\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/55cdq9mj5msby1f05d6s0r3m0000gp/T/ipykernel_10434/233808433.py:7: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';', nrows=5000)\n",
      "/var/folders/x9/55cdq9mj5msby1f05d6s0r3m0000gp/T/ipykernel_10434/233808433.py:8: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  customers = pd.read_csv('data/Udacity_CUSTOMERS_052018.csv', sep=';', nrows=5000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved as 'missing_data_analysis.png'\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load first 5000 rows for better sample\n",
    "print(\"Loading data...\")\n",
    "azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';', nrows=5000)\n",
    "customers = pd.read_csv('data/Udacity_CUSTOMERS_052018.csv', sep=';', nrows=5000)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 1: IDENTIFY OBJECT (STRING) COLUMNS\")\n",
    "print(\"=\"*70)\n",
    "object_cols = azdias.select_dtypes(include=['object']).columns\n",
    "print(f\"Object columns: {object_cols.tolist()}\")\n",
    "\n",
    "for col in object_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Unique values: {azdias[col].unique()[:20]}\")\n",
    "    print(f\"  Value counts:\\n{azdias[col].value_counts().head(10)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: CHECK FOR -1, 0, X PATTERNS (Potential Missing Codes)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check numeric columns for suspicious values\n",
    "suspicious_cols = []\n",
    "for col in azdias.columns[:50]:  # Check first 50 columns\n",
    "    if azdias[col].dtype in ['int64', 'float64']:\n",
    "        unique_vals = azdias[col].dropna().unique()\n",
    "        \n",
    "        # Check for -1\n",
    "        if -1 in unique_vals:\n",
    "            count_neg1 = (azdias[col] == -1).sum()\n",
    "            suspicious_cols.append((col, '-1', count_neg1))\n",
    "        \n",
    "        # Check for 0 (if it seems like it might be a missing code)\n",
    "        if 0 in unique_vals:\n",
    "            count_0 = (azdias[col] == 0).sum()\n",
    "            # If more than 30% are zeros, it might be suspicious\n",
    "            if count_0 / len(azdias) > 0.3:\n",
    "                suspicious_cols.append((col, '0', count_0))\n",
    "\n",
    "print(f\"Found {len(suspicious_cols)} columns with suspicious values:\")\n",
    "for col, value, count in suspicious_cols[:20]:\n",
    "    print(f\"  {col}: {value} appears {count} times ({count/len(azdias)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: MISSING VALUE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'column': azdias.columns,\n",
    "    'missing_count': azdias.isnull().sum(),\n",
    "    'missing_percent': (azdias.isnull().sum() / len(azdias) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_summary = missing_summary[missing_summary['missing_count'] > 0].sort_values(\n",
    "    'missing_percent', ascending=False\n",
    ")\n",
    "\n",
    "print(f\"\\nColumns with >50% missing values: {len(missing_summary[missing_summary['missing_percent'] > 50])}\")\n",
    "print(f\"Columns with >80% missing values: {len(missing_summary[missing_summary['missing_percent'] > 80])}\")\n",
    "\n",
    "print(\"\\nTop 30 columns by missing percentage:\")\n",
    "print(missing_summary.head(30))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: COMPARE AZDIAS vs CUSTOMERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if CUSTOMERS has the 3 extra columns\n",
    "extra_cols = set(customers.columns) - set(azdias.columns)\n",
    "print(f\"Extra columns in CUSTOMERS: {extra_cols}\")\n",
    "\n",
    "if extra_cols:\n",
    "    for col in extra_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(customers[col].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: VISUALIZE MISSING DATA PATTERN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate missing percentage for each column\n",
    "missing_pct = (azdias.isnull().sum() / len(azdias) * 100).sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(missing_pct[:30])), missing_pct[:30])\n",
    "plt.axhline(y=50, color='r', linestyle='--', label='50% threshold')\n",
    "plt.axhline(y=80, color='orange', linestyle='--', label='80% threshold')\n",
    "plt.xlabel('Column Index')\n",
    "plt.ylabel('Missing Percentage (%)')\n",
    "plt.title('Top 30 Columns with Missing Data')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('missing_data_analysis.png', dpi=100, bbox_inches='tight')\n",
    "print(\"Plot saved as 'missing_data_analysis.png'\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_demographics_data(df):\n",
    "    \"\"\"\n",
    "    Clean German demographics data based on official DIAS documentation\n",
    "    \n",
    "    Missing value codes:\n",
    "    - -1: unknown (universal)\n",
    "    - 0: unknown (context-dependent - check feature by feature)\n",
    "    - 9: unknown (specific features like KBA05_, SEMIO_)\n",
    "    - 'X': unknown (CAMEO columns)\n",
    "    \"\"\"\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 1: Universal -1 = unknown\n",
    "    # ========================================\n",
    "    print(\"Step 1: Converting -1 to NaN...\")\n",
    "    df_clean = df_clean.replace(-1, np.nan)\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 2: Feature-specific 0 = unknown\n",
    "    # ========================================\n",
    "    print(\"Step 2: Converting specific 0 values to NaN...\")\n",
    "    \n",
    "    # Features where 0 = unknown/no classification\n",
    "    zero_is_missing = [\n",
    "        'AGER_TYP',\n",
    "        'ALTER_HH', \n",
    "        'ALTERSKATEGORIE_GROB',\n",
    "        'ANREDE_KZ',\n",
    "        'BIP_FLAG',\n",
    "        'GEBAEUDETYP',\n",
    "        'GEBAEUDETYP_RASTER',\n",
    "        'GEOSCORE_KLS7',\n",
    "        'HAUSHALTSSTRUKTUR',\n",
    "        'HEALTH_TYP',\n",
    "        'HH_EINKOMMEN_SCORE',\n",
    "        'KBA05_BAUMAX',\n",
    "        'KBA05_GBZ',\n",
    "        'KKK',\n",
    "        'LP_FAMILIE_GROB',\n",
    "        'LP_STATUS_GROB',\n",
    "        'NATIONALITAET_KZ',\n",
    "        'ONLINE_AFFINITAET',\n",
    "        'PRAEGENDE_JUGENDJAHRE',\n",
    "        'REGIOTYP',\n",
    "        'RETOURTYP_BK_S',\n",
    "        'TITEL_KZ',\n",
    "        'WOHNDAUER_2008',\n",
    "        'WOHNLAGE',\n",
    "        'WACHSTUMSGEBIET_NB',\n",
    "        'W_KEIT_KIND_HH'\n",
    "    ]\n",
    "    \n",
    "    for col in zero_is_missing:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].replace(0, np.nan)\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 3: D19 columns - 0 = \"no transaction known\"\n",
    "    # ========================================\n",
    "    print(\"Step 3: Converting D19 transaction 0's to NaN...\")\n",
    "    \n",
    "    # All D19 columns where 0 = \"no transaction known\"\n",
    "    d19_cols = [col for col in df_clean.columns if col.startswith('D19_')]\n",
    "    \n",
    "    for col in d19_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].replace(0, np.nan)\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 4: Feature-specific 9 = unknown\n",
    "    # ========================================\n",
    "    print(\"Step 4: Converting 9 to NaN for specific columns...\")\n",
    "    \n",
    "    # KBA05 columns where 9 = unknown\n",
    "    kba05_cols = [col for col in df_clean.columns if col.startswith('KBA05_')]\n",
    "    for col in kba05_cols:\n",
    "        df_clean[col] = df_clean[col].replace(9, np.nan)\n",
    "    \n",
    "    # SEMIO columns where 9 = unknown\n",
    "    semio_cols = [col for col in df_clean.columns if col.startswith('SEMIO_')]\n",
    "    for col in semio_cols:\n",
    "        df_clean[col] = df_clean[col].replace(9, np.nan)\n",
    "    \n",
    "    # Other columns with 9 = unknown\n",
    "    nine_is_missing = [\n",
    "        'ALTERSKATEGORIE_GROB',\n",
    "        'RELAT_AB',\n",
    "        'ZABEOTYP'\n",
    "    ]\n",
    "    \n",
    "    for col in nine_is_missing:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].replace(9, np.nan)\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 5: Handle mixed-type CAMEO columns\n",
    "    # ========================================\n",
    "    print(\"Step 5: Fixing CAMEO columns...\")\n",
    "    \n",
    "    # CAMEO_DEUG_2015: Mixed types (floats and strings) + 'X'\n",
    "    if 'CAMEO_DEUG_2015' in df_clean.columns:\n",
    "        df_clean['CAMEO_DEUG_2015'] = df_clean['CAMEO_DEUG_2015'].replace('X', np.nan)\n",
    "        df_clean['CAMEO_DEUG_2015'] = pd.to_numeric(df_clean['CAMEO_DEUG_2015'], errors='coerce')\n",
    "    \n",
    "    # CAMEO_INTL_2015: Same issue\n",
    "    if 'CAMEO_INTL_2015' in df_clean.columns:\n",
    "        df_clean['CAMEO_INTL_2015'] = pd.to_numeric(df_clean['CAMEO_INTL_2015'], errors='coerce')\n",
    "    \n",
    "    # CAMEO_DEU_2015: Keep as categorical (alphanumeric codes like '8A')\n",
    "    # No changes needed - it's legitimately categorical\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 6: Handle D19_LETZTER_KAUF_BRANCHE\n",
    "    # ========================================\n",
    "    print(\"Step 6: Converting 'D19_UNBEKANNT' to NaN...\")\n",
    "    \n",
    "    if 'D19_LETZTER_KAUF_BRANCHE' in df_clean.columns:\n",
    "        df_clean['D19_LETZTER_KAUF_BRANCHE'] = df_clean['D19_LETZTER_KAUF_BRANCHE'].replace(\n",
    "            'D19_UNBEKANNT', np.nan\n",
    "        )\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 7: Convert EINGEFUEGT_AM to datetime\n",
    "    # ========================================\n",
    "    print(\"Step 7: Converting dates...\")\n",
    "    \n",
    "    if 'EINGEFUEGT_AM' in df_clean.columns:\n",
    "        df_clean['EINGEFUEGT_AM'] = pd.to_datetime(df_clean['EINGEFUEGT_AM'], errors='coerce')\n",
    "    \n",
    "    print(\"Data cleaning complete!\")\n",
    "    print(f\"Shape after cleaning: {df_clean.shape}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def get_missing_summary(df):\n",
    "    \"\"\"Get comprehensive missing value summary\"\"\"\n",
    "    missing = pd.DataFrame({\n",
    "        'column': df.columns,\n",
    "        'missing_count': df.isnull().sum(),\n",
    "        'missing_percent': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "        'dtype': df.dtypes\n",
    "    })\n",
    "    \n",
    "    missing = missing[missing['missing_count'] > 0].sort_values(\n",
    "        'missing_percent', ascending=False\n",
    "    )\n",
    "    \n",
    "    return missing\n",
    "\n",
    "\n",
    "def drop_high_missing_columns(df, threshold=80):\n",
    "    \"\"\"Drop columns with more than threshold% missing\"\"\"\n",
    "    missing_pct = (df.isnull().sum() / len(df) * 100)\n",
    "    cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
    "    \n",
    "    print(f\"Dropping {len(cols_to_drop)} columns with >{threshold}% missing:\")\n",
    "    print(cols_to_drop)\n",
    "    \n",
    "    df_reduced = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    print(f\"Shape: {df.shape} → {df_reduced.shape}\")\n",
    "    \n",
    "    return df_reduced, cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/55cdq9mj5msby1f05d6s0r3m0000gp/T/ipykernel_10434/3976963912.py:3: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';')\n",
      "/var/folders/x9/55cdq9mj5msby1f05d6s0r3m0000gp/T/ipykernel_10434/3976963912.py:4: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  customers = pd.read_csv('data/Udacity_CUSTOMERS_052018.csv', sep=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZDIAS shape: (891221, 366)\n",
      "CUSTOMERS shape: (191652, 369)\n",
      "\n",
      "======================================================================\n",
      "CLEANING AZDIAS\n",
      "======================================================================\n",
      "Step 1: Converting -1 to NaN...\n",
      "Step 2: Converting specific 0 values to NaN...\n",
      "Step 3: Converting D19 transaction 0's to NaN...\n",
      "Step 4: Converting 9 to NaN for specific columns...\n",
      "Step 5: Fixing CAMEO columns...\n",
      "Step 6: Converting 'D19_UNBEKANNT' to NaN...\n",
      "Step 7: Converting dates...\n",
      "Data cleaning complete!\n",
      "Shape after cleaning: (891221, 366)\n",
      "\n",
      "======================================================================\n",
      "CLEANING CUSTOMERS\n",
      "======================================================================\n",
      "Step 1: Converting -1 to NaN...\n",
      "Step 2: Converting specific 0 values to NaN...\n",
      "Step 3: Converting D19 transaction 0's to NaN...\n",
      "Step 4: Converting 9 to NaN for specific columns...\n",
      "Step 5: Fixing CAMEO columns...\n",
      "Step 6: Converting 'D19_UNBEKANNT' to NaN...\n",
      "Step 7: Converting dates...\n",
      "Data cleaning complete!\n",
      "Shape after cleaning: (191652, 369)\n",
      "\n",
      "======================================================================\n",
      "MISSING VALUES AFTER CLEANING\n",
      "======================================================================\n",
      "                                                column  missing_count  \\\n",
      "D19_TELKO_ONLINE_QUOTE_12    D19_TELKO_ONLINE_QUOTE_12         890433   \n",
      "ALTER_KIND4                                ALTER_KIND4         890016   \n",
      "D19_VERSI_ONLINE_QUOTE_12    D19_VERSI_ONLINE_QUOTE_12         889575   \n",
      "TITEL_KZ                                      TITEL_KZ         889061   \n",
      "ALTER_KIND3                                ALTER_KIND3         885051   \n",
      "D19_BANKEN_LOKAL                      D19_BANKEN_LOKAL         874745   \n",
      "ALTER_KIND2                                ALTER_KIND2         861722   \n",
      "D19_TELKO_ANZ_12                      D19_TELKO_ANZ_12         857990   \n",
      "D19_DIGIT_SERV                          D19_DIGIT_SERV         857661   \n",
      "D19_BIO_OEKO                              D19_BIO_OEKO         854074   \n",
      "D19_TIERARTIKEL                        D19_TIERARTIKEL         852220   \n",
      "D19_NAHRUNGSERGAENZUNG          D19_NAHRUNGSERGAENZUNG         852176   \n",
      "D19_GARTEN                                  D19_GARTEN         851626   \n",
      "D19_BANKEN_ONLINE_QUOTE_12  D19_BANKEN_ONLINE_QUOTE_12         845987   \n",
      "D19_LEBENSMITTEL                      D19_LEBENSMITTEL         837914   \n",
      "D19_WEIN_FEINKOST                    D19_WEIN_FEINKOST         836142   \n",
      "D19_BANKEN_ANZ_12                    D19_BANKEN_ANZ_12         831734   \n",
      "D19_ENERGIE                                D19_ENERGIE         829857   \n",
      "D19_TELKO_ANZ_24                      D19_TELKO_ANZ_24         826208   \n",
      "D19_BANKEN_REST                        D19_BANKEN_REST         821760   \n",
      "D19_VERSI_ANZ_12                      D19_VERSI_ANZ_12         821289   \n",
      "D19_BILDUNG                                D19_BILDUNG         813156   \n",
      "ALTER_KIND1                                ALTER_KIND1         810163   \n",
      "D19_BEKLEIDUNG_GEH                  D19_BEKLEIDUNG_GEH         809304   \n",
      "D19_RATGEBER                              D19_RATGEBER         805071   \n",
      "D19_SAMMELARTIKEL                    D19_SAMMELARTIKEL         802085   \n",
      "D19_BANKEN_ANZ_24                    D19_BANKEN_ANZ_24         794100   \n",
      "D19_FREIZEIT                              D19_FREIZEIT         790748   \n",
      "D19_BANKEN_GROSS                      D19_BANKEN_GROSS         785351   \n",
      "D19_VERSI_ANZ_24                      D19_VERSI_ANZ_24         777037   \n",
      "\n",
      "                            missing_percent    dtype  \n",
      "D19_TELKO_ONLINE_QUOTE_12             99.91  float64  \n",
      "ALTER_KIND4                           99.86  float64  \n",
      "D19_VERSI_ONLINE_QUOTE_12             99.82  float64  \n",
      "TITEL_KZ                              99.76  float64  \n",
      "ALTER_KIND3                           99.31  float64  \n",
      "D19_BANKEN_LOKAL                      98.15  float64  \n",
      "ALTER_KIND2                           96.69  float64  \n",
      "D19_TELKO_ANZ_12                      96.27  float64  \n",
      "D19_DIGIT_SERV                        96.23  float64  \n",
      "D19_BIO_OEKO                          95.83  float64  \n",
      "D19_TIERARTIKEL                       95.62  float64  \n",
      "D19_NAHRUNGSERGAENZUNG                95.62  float64  \n",
      "D19_GARTEN                            95.56  float64  \n",
      "D19_BANKEN_ONLINE_QUOTE_12            94.92  float64  \n",
      "D19_LEBENSMITTEL                      94.02  float64  \n",
      "D19_WEIN_FEINKOST                     93.82  float64  \n",
      "D19_BANKEN_ANZ_12                     93.33  float64  \n",
      "D19_ENERGIE                           93.11  float64  \n",
      "D19_TELKO_ANZ_24                      92.71  float64  \n",
      "D19_BANKEN_REST                       92.21  float64  \n",
      "D19_VERSI_ANZ_12                      92.15  float64  \n",
      "D19_BILDUNG                           91.24  float64  \n",
      "ALTER_KIND1                           90.90  float64  \n",
      "D19_BEKLEIDUNG_GEH                    90.81  float64  \n",
      "D19_RATGEBER                          90.33  float64  \n",
      "D19_SAMMELARTIKEL                     90.00  float64  \n",
      "D19_BANKEN_ANZ_24                     89.10  float64  \n",
      "D19_FREIZEIT                          88.73  float64  \n",
      "D19_BANKEN_GROSS                      88.12  float64  \n",
      "D19_VERSI_ANZ_24                      87.19  float64  \n",
      "\n",
      "======================================================================\n",
      "DROPPING HIGH-MISSING COLUMNS\n",
      "======================================================================\n",
      "Dropping 43 columns with >80% missing:\n",
      "['ALTER_KIND1', 'ALTER_KIND2', 'ALTER_KIND3', 'ALTER_KIND4', 'D19_BANKEN_ANZ_12', 'D19_BANKEN_ANZ_24', 'D19_BANKEN_DIREKT', 'D19_BANKEN_GROSS', 'D19_BANKEN_LOKAL', 'D19_BANKEN_ONLINE_QUOTE_12', 'D19_BANKEN_REST', 'D19_BEKLEIDUNG_GEH', 'D19_BILDUNG', 'D19_BIO_OEKO', 'D19_DIGIT_SERV', 'D19_DROGERIEARTIKEL', 'D19_ENERGIE', 'D19_FREIZEIT', 'D19_GARTEN', 'D19_HANDWERK', 'D19_HAUS_DEKO', 'D19_KINDERARTIKEL', 'D19_KOSMETIK', 'D19_LEBENSMITTEL', 'D19_LOTTO', 'D19_NAHRUNGSERGAENZUNG', 'D19_RATGEBER', 'D19_REISEN', 'D19_SAMMELARTIKEL', 'D19_SCHUHE', 'D19_SOZIALES', 'D19_TELKO_ANZ_12', 'D19_TELKO_ANZ_24', 'D19_TELKO_MOBILE', 'D19_TELKO_ONLINE_QUOTE_12', 'D19_TELKO_REST', 'D19_TIERARTIKEL', 'D19_VERSAND_REST', 'D19_VERSI_ANZ_12', 'D19_VERSI_ANZ_24', 'D19_VERSI_ONLINE_QUOTE_12', 'D19_WEIN_FEINKOST', 'TITEL_KZ']\n",
      "Shape: (891221, 366) → (891221, 323)\n",
      "Dropping 36 columns with >80% missing:\n",
      "['ALTER_KIND1', 'ALTER_KIND2', 'ALTER_KIND3', 'ALTER_KIND4', 'D19_BANKEN_ANZ_12', 'D19_BANKEN_ANZ_24', 'D19_BANKEN_DIREKT', 'D19_BANKEN_GROSS', 'D19_BANKEN_LOKAL', 'D19_BANKEN_ONLINE_QUOTE_12', 'D19_BANKEN_REST', 'D19_BEKLEIDUNG_GEH', 'D19_BILDUNG', 'D19_BIO_OEKO', 'D19_DIGIT_SERV', 'D19_DROGERIEARTIKEL', 'D19_ENERGIE', 'D19_FREIZEIT', 'D19_GARTEN', 'D19_KINDERARTIKEL', 'D19_LEBENSMITTEL', 'D19_NAHRUNGSERGAENZUNG', 'D19_RATGEBER', 'D19_SCHUHE', 'D19_TELKO_ANZ_12', 'D19_TELKO_ANZ_24', 'D19_TELKO_MOBILE', 'D19_TELKO_ONLINE_QUOTE_12', 'D19_TELKO_REST', 'D19_TIERARTIKEL', 'D19_VERSAND_REST', 'D19_VERSI_ANZ_12', 'D19_VERSI_ANZ_24', 'D19_VERSI_ONLINE_QUOTE_12', 'D19_WEIN_FEINKOST', 'TITEL_KZ']\n",
      "Shape: (191652, 369) → (191652, 333)\n",
      "\n",
      "Final shapes:\n",
      "AZDIAS: (891221, 323)\n",
      "CUSTOMERS: (191652, 333)\n"
     ]
    }
   ],
   "source": [
    "# Load full datasets (not just 5000 rows)\n",
    "print(\"Loading full datasets...\")\n",
    "azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "customers = pd.read_csv('data/Udacity_CUSTOMERS_052018.csv', sep=';')\n",
    "\n",
    "print(f\"AZDIAS shape: {azdias.shape}\")\n",
    "print(f\"CUSTOMERS shape: {customers.shape}\")\n",
    "\n",
    "# Clean the data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING AZDIAS\")\n",
    "print(\"=\"*70)\n",
    "azdias_clean = clean_demographics_data(azdias)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING CUSTOMERS\")\n",
    "print(\"=\"*70)\n",
    "customers_clean = clean_demographics_data(customers)\n",
    "\n",
    "# Get missing summary AFTER cleaning\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MISSING VALUES AFTER CLEANING\")\n",
    "print(\"=\"*70)\n",
    "missing_summary = get_missing_summary(azdias_clean)\n",
    "print(missing_summary.head(30))\n",
    "\n",
    "# Drop columns with >80% missing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DROPPING HIGH-MISSING COLUMNS\")\n",
    "print(\"=\"*70)\n",
    "azdias_reduced, dropped_cols = drop_high_missing_columns(azdias_clean, threshold=80)\n",
    "customers_reduced, _ = drop_high_missing_columns(customers_clean, threshold=80)\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"AZDIAS: {azdias_reduced.shape}\")\n",
    "print(f\"CUSTOMERS: {customers_reduced.shape}\")\n",
    "\n",
    "# Save cleaned datasets\n",
    "azdias_reduced.to_csv('data/cleaned_AZDIAS.csv', index=False)\n",
    "customers_reduced.to_csv('data/cleaned_CUSTOMERS.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_reduced.to_csv('data/cleaned_AZDIAS.csv', index=False)\n",
    "customers_reduced.to_csv('data/cleaned_CUSTOMERS.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to add in a lot more cells (both markdown and code) to document your\n",
    "# approach and findings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
